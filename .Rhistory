mutate(name=str_to_lower(name)) %>%
group_by(sex,name) %>%
summarise(n=sum(n)) %>%
ungroup
condition_distribution = function(context_size = 3L){
param_n = context_size + 1
pad = paste(rep(" ",context_size),collapse="")
nm = name_data %>% mutate(name=paste0(pad,name,"#"))
ngrams = nm %>%
mutate(ngram=tokenize_character_shingles(
name, n = param_n, strip_non_alphanum = FALSE)) %>%
unnest %>%
group_by(sex,ngram) %>%
summarise(cnt=sum(n))
ngrams %>%
mutate(prev=str_sub(ngram,1,context_size),
nxt=str_sub(ngram,param_n,param_n)) %>%
group_by(sex,prev,nxt) %>%
summarise(joint_count=sum(cnt)) %>%
mutate(pct=joint_count/sum(joint_count)) %>%
ungroup %>%
select(-joint_count)
}
optimize_conditional_distribution = function(conditional){
conditional %>%
split(.$sex) %>%
map(~split(., .$prev)) %>%
map(list2env)
}
next_letter_dist=function(conditional,past,sex){
df=conditional[[sex]][[past]]
if(is.null(df))
return(data_frame(nxt="#",pct=0))
dplyr::select(df,nxt,pct)
}
generator = function(context_size){
cond=condition_distribution(context_size)
cond=optimize_conditional_distribution(cond)
function(sex=NULL,starter=NULL){
if(is.null(starter))
starter=paste(rep(" ",context_size),collapse="")
if(is.null(sex))
sex=sample(c("M","F","U"),1L)
name_so_far = starter
temp = next_letter_dist(cond,str_sub(name_so_far,-context_size),sex)
next_char = sample(temp$nxt,size=1L,prob=temp$pct)
while(next_char != "#"){
name_so_far=paste0(name_so_far,next_char)
temp=next_letter_dist(cond,str_sub(name_so_far,-context_size),sex)
next_char=sample(temp$nxt,size=1L,prob=temp$pct)
}
str_trim(name_so_far)
}
}
gen = generator(3L)
gen3_sample = replicate(25,gen())
gen3_sample
library(magrittr)
library(dplyr)
library(stringr)
library(tidyr)
library(purrr)
library(tokenizers)
library(microbenchmark)
setwd("C:/Users/bridg/Documents")
names = read.csv("allnames.csv",header=TRUE)
name=names$ï..name
sex=names$sex
name_data = names %>%
mutate(name=str_to_lower(name)) %>%
group_by(sex,name) %>%
summarise(n=sum(n)) %>%
ungroup
condition_distribution = function(context_size = 3L){
param_n = context_size + 1
pad = paste(rep(" ",context_size),collapse="")
nm = name_data %>% mutate(name=paste0(pad,name,"#"))
ngrams = nm %>%
mutate(ngram=tokenize_character_shingles(
name, n = param_n, strip_non_alphanum = FALSE)) %>%
unnest %>%
group_by(sex,ngram) %>%
summarise(cnt=sum(n))
ngrams %>%
mutate(prev=str_sub(ngram,1,context_size),
nxt=str_sub(ngram,param_n,param_n)) %>%
group_by(sex,prev,nxt) %>%
summarise(joint_count=sum(cnt)) %>%
mutate(pct=joint_count/sum(joint_count)) %>%
ungroup %>%
select(-joint_count)
}
optimize_conditional_distribution = function(conditional){
conditional %>%
split(.$sex) %>%
map(~split(., .$prev)) %>%
map(list2env)
}
next_letter_dist=function(conditional,past,sex){
df=conditional[[sex]][[past]]
if(is.null(df))
return(data_frame(nxt="#",pct=0))
dplyr::select(df,nxt,pct)
}
generator = function(context_size){
cond=condition_distribution(context_size)
cond=optimize_conditional_distribution(cond)
function(sex=NULL,starter=NULL){
if(is.null(starter))
starter=paste(rep(" ",context_size),collapse="")
if(is.null(sex))
sex=sample(c("M","F","U"),1L)
name_so_far = starter
temp = next_letter_dist(cond,str_sub(name_so_far,-context_size),sex)
next_char = sample(temp$nxt,size=1L,prob=temp$pct)
while(next_char != "#"){
name_so_far=paste0(name_so_far,next_char)
temp=next_letter_dist(cond,str_sub(name_so_far,-context_size),sex)
next_char=sample(temp$nxt,size=1L,prob=temp$pct)
}
str_trim(name_so_far)
}
}
gen = generator(3L)
gen3_sample = replicate(25,gen())
gen3_sample
library(magrittr)
library(dplyr)
library(stringr)
library(tidyr)
library(purrr)
library(tokenizers)
library(microbenchmark)
setwd("C:/Users/bridg/Documents")
names = read.csv("allnames.csv",header=TRUE)
name=names$ï..name
sex=names$sex
name_data = names %>%
mutate(name=str_to_lower(name)) %>%
group_by(sex,name) %>%
summarise(n=sum(n)) %>%
ungroup
condition_distribution = function(context_size = 3L){
param_n = context_size + 1
pad = paste(rep(" ",context_size),collapse="")
nm = name_data %>% mutate(name=paste0(pad,name,"#"))
ngrams = nm %>%
mutate(ngram=tokenize_character_shingles(
name, n = param_n, strip_non_alphanum = FALSE)) %>%
unnest %>%
group_by(sex,ngram) %>%
summarise(cnt=sum(n))
ngrams %>%
mutate(prev=str_sub(ngram,1,context_size),
nxt=str_sub(ngram,param_n,param_n)) %>%
group_by(sex,prev,nxt) %>%
summarise(joint_count=sum(cnt)) %>%
mutate(pct=joint_count/sum(joint_count)) %>%
ungroup %>%
select(-joint_count)
}
optimize_conditional_distribution = function(conditional){
conditional %>%
split(.$sex) %>%
map(~split(., .$prev)) %>%
map(list2env)
}
next_letter_dist=function(conditional,past,sex){
df=conditional[[sex]][[past]]
if(is.null(df))
return(data_frame(nxt="#",pct=0))
dplyr::select(df,nxt,pct)
}
generator = function(context_size){
cond=condition_distribution(context_size)
cond=optimize_conditional_distribution(cond)
function(sex=NULL,starter=NULL){
if(is.null(starter))
starter=paste(rep(" ",context_size),collapse="")
if(is.null(sex))
sex=sample(c("M","F","U"),1L)
name_so_far = starter
temp = next_letter_dist(cond,str_sub(name_so_far,-context_size),sex)
next_char = sample(temp$nxt,size=1L,prob=temp$pct)
while(next_char != "#"){
name_so_far=paste0(name_so_far,next_char)
temp=next_letter_dist(cond,str_sub(name_so_far,-context_size),sex)
next_char=sample(temp$nxt,size=1L,prob=temp$pct)
}
str_trim(name_so_far)
}
}
gen = generator(3L)
gen3_sample = replicate(25,gen())
gen3_sample
library(magrittr)
library(dplyr)
library(stringr)
library(tidyr)
library(purrr)
library(tokenizers)
library(microbenchmark)
setwd("C:/Users/bridg/Documents")
names = read.csv("allnames.csv",header=TRUE)
name=names$ï..name
sex=names$sex
name_data = names %>%
mutate(name=str_to_lower(name)) %>%
group_by(sex,name) %>%
summarise(n=sum(n)) %>%
ungroup
condition_distribution = function(context_size = 3L){
param_n = context_size + 1
pad = paste(rep(" ",context_size),collapse="")
nm = name_data %>% mutate(name=paste0(pad,name,"#"))
ngrams = nm %>%
mutate(ngram=tokenize_character_shingles(
name, n = param_n, strip_non_alphanum = FALSE)) %>%
unnest %>%
group_by(sex,ngram) %>%
summarise(cnt=sum(n))
ngrams %>%
mutate(prev=str_sub(ngram,1,context_size),
nxt=str_sub(ngram,param_n,param_n)) %>%
group_by(sex,prev,nxt) %>%
summarise(joint_count=sum(cnt)) %>%
mutate(pct=joint_count/sum(joint_count)) %>%
ungroup %>%
select(-joint_count)
}
optimize_conditional_distribution = function(conditional){
conditional %>%
split(.$sex) %>%
map(~split(., .$prev)) %>%
map(list2env)
}
next_letter_dist=function(conditional,past,sex){
df=conditional[[sex]][[past]]
if(is.null(df))
return(data_frame(nxt="#",pct=0))
dplyr::select(df,nxt,pct)
}
generator = function(context_size){
cond=condition_distribution(context_size)
cond=optimize_conditional_distribution(cond)
function(sex=NULL,starter=NULL){
if(is.null(starter))
starter=paste(rep(" ",context_size),collapse="")
if(is.null(sex))
sex=sample(c("M","F","U"),1L)
name_so_far = starter
temp = next_letter_dist(cond,str_sub(name_so_far,-context_size),sex)
next_char = sample(temp$nxt,size=1L,prob=temp$pct)
while(next_char != "#"){
name_so_far=paste0(name_so_far,next_char)
temp=next_letter_dist(cond,str_sub(name_so_far,-context_size),sex)
next_char=sample(temp$nxt,size=1L,prob=temp$pct)
}
str_trim(name_so_far)
}
}
gen = generator(3L)
gen3_sample = replicate(25,gen())
gen3_sample
library(magrittr)
library(dplyr)
library(stringr)
library(tidyr)
library(purrr)
library(tokenizers)
library(microbenchmark)
setwd("C:/Users/bridg/Documents")
names = read.csv("allnames.csv",header=TRUE)
name=names$ï..name
sex=names$sex
name_data = names %>%
mutate(name=str_to_lower(name)) %>%
group_by(sex,name) %>%
summarise(n=sum(n)) %>%
ungroup
condition_distribution = function(context_size = 3L){
param_n = context_size + 1
pad = paste(rep(" ",context_size),collapse="")
nm = name_data %>% mutate(name=paste0(pad,name,"#"))
ngrams = nm %>%
mutate(ngram=tokenize_character_shingles(
name, n = param_n, strip_non_alphanum = FALSE)) %>%
unnest %>%
group_by(sex,ngram) %>%
summarise(cnt=sum(n))
ngrams %>%
mutate(prev=str_sub(ngram,1,context_size),
nxt=str_sub(ngram,param_n,param_n)) %>%
group_by(sex,prev,nxt) %>%
summarise(joint_count=sum(cnt)) %>%
mutate(pct=joint_count/sum(joint_count)) %>%
ungroup %>%
select(-joint_count)
}
optimize_conditional_distribution = function(conditional){
conditional %>%
split(.$sex) %>%
map(~split(., .$prev)) %>%
map(list2env)
}
next_letter_dist=function(conditional,past,sex){
df=conditional[[sex]][[past]]
if(is.null(df))
return(data_frame(nxt="#",pct=0))
dplyr::select(df,nxt,pct)
}
generator = function(context_size){
cond=condition_distribution(context_size)
cond=optimize_conditional_distribution(cond)
function(sex=NULL,starter=NULL){
if(is.null(starter))
starter=paste(rep(" ",context_size),collapse="")
if(is.null(sex))
sex=sample(c("M","F","U"),1L)
name_so_far = starter
temp = next_letter_dist(cond,str_sub(name_so_far,-context_size),sex)
next_char = sample(temp$nxt,size=1L,prob=temp$pct)
while(next_char != "#"){
name_so_far=paste0(name_so_far,next_char)
temp=next_letter_dist(cond,str_sub(name_so_far,-context_size),sex)
next_char=sample(temp$nxt,size=1L,prob=temp$pct)
}
str_trim(name_so_far)
}
}
gen = generator(3L)
gen3_sample = replicate(25,gen())
gen3_sample
library(magrittr)
library(dplyr)
library(stringr)
library(tidyr)
library(purrr)
library(tokenizers)
library(microbenchmark)
setwd("C:/Users/bridg/Documents")
names = read.csv("allnames.csv",header=TRUE)
name=names$ï..name
sex=names$sex
name_data = names %>%
mutate(name=str_to_lower(name)) %>%
group_by(sex,name) %>%
summarise(n=sum(n)) %>%
ungroup
condition_distribution = function(context_size = 3L){
param_n = context_size + 1
pad = paste(rep(" ",context_size),collapse="")
nm = name_data %>% mutate(name=paste0(pad,name,"#"))
ngrams = nm %>%
mutate(ngram=tokenize_character_shingles(
name, n = param_n, strip_non_alphanum = FALSE)) %>%
unnest %>%
group_by(sex,ngram) %>%
summarise(cnt=sum(n))
ngrams %>%
mutate(prev=str_sub(ngram,1,context_size),
nxt=str_sub(ngram,param_n,param_n)) %>%
group_by(sex,prev,nxt) %>%
summarise(joint_count=sum(cnt)) %>%
mutate(pct=joint_count/sum(joint_count)) %>%
ungroup %>%
select(-joint_count)
}
optimize_conditional_distribution = function(conditional){
conditional %>%
split(.$sex) %>%
map(~split(., .$prev)) %>%
map(list2env)
}
next_letter_dist=function(conditional,past,sex){
df=conditional[[sex]][[past]]
if(is.null(df))
return(data_frame(nxt="#",pct=0))
dplyr::select(df,nxt,pct)
}
generator = function(context_size){
cond=condition_distribution(context_size)
cond=optimize_conditional_distribution(cond)
function(sex=NULL,starter=NULL){
if(is.null(starter))
starter=paste(rep(" ",context_size),collapse="")
if(is.null(sex))
sex=sample(c("M","F","U"),1L)
name_so_far = starter
temp = next_letter_dist(cond,str_sub(name_so_far,-context_size),sex)
next_char = sample(temp$nxt,size=1L,prob=temp$pct)
while(next_char != "#"){
name_so_far=paste0(name_so_far,next_char)
temp=next_letter_dist(cond,str_sub(name_so_far,-context_size),sex)
next_char=sample(temp$nxt,size=1L,prob=temp$pct)
}
str_trim(name_so_far)
}
}
gen = generator(3L)
gen3_sample = replicate(25,gen())
gen3_sample
getwd()
setwd("C:/Users/bridg/Documents/AMS598")
getwd()
ls
data = read.csv("project1.csv")
data = read.csv("project1_data.csv")
data
hist(data$y)
reg = lm(data$y~data$X1+data$X2+data$X3+data$X4+data$X5+data$X6+data$X7+data$X8+data$X9+data$X10)
summary(reg)
confint(reg)
install.packages("boot")
library(Boot)
library(boot)
cof = function(dat, indices){}
cof = function(dat, indices){
dt = dat[indices,]
reg = summary(lm(data$y~data$X1+data$X2+data$X3+data$X4+data$X5+data$X6+data$X7+data$X8+data$X9+data$X10))
co = reg$coefficients
return(co)
}
summary(reg)$coefficients
coef(summary(reg))
C1=coef(summary(reg))[1,2]
C1
C1=coef(summary(reg))[2,1]
C1
library(boot)
mydata=read.csv("project1_data.csv",header=TRUE)
cof=function(data, indices){
dt=data[indices,]
reg=summary(lm(data$y~data$X1+data$X2+data$X3+data$X4+data$X5+data$X6+data$X7+data$X8+data$X9+data$X10))
C1=coef(reg)[2,1]
C2=coef(reg)[3,1]
C3=coef(reg)[4,1]
C4=coef(reg)[5,1]
C5=coef(reg)[6,1]
C6=coef(reg)[7,1]
C7=coef(reg)[8,1]
C8=coef(reg)[9,1]
C9=coef(reg)[10,1]
C10=coef(reg)[11,1]
coefs=c(C1,C2,C3,C4,C5,C6,C7,C8,C9,C10)
return(coefs)
}
myboot = boot(mydata,cof,R=5)
myboot
myboot$t0
library(boot)
mydata=read.csv("project1_data.csv",header=TRUE)
cof=function(data, indices){
dt=data[indices,]
reg=summary(lm(data$y~data$X1+data$X2+data$X3+data$X4+data$X5+data$X6+data$X7+data$X8+data$X9+data$X10))
C1=coef(reg)[2,1]
C2=coef(reg)[3,1]
C3=coef(reg)[4,1]
C4=coef(reg)[5,1]
C5=coef(reg)[6,1]
C6=coef(reg)[7,1]
C7=coef(reg)[8,1]
C8=coef(reg)[9,1]
C9=coef(reg)[10,1]
C10=coef(reg)[11,1]
coefs=c(C1,C2,C3,C4,C5,C6,C7,C8,C9,C10)
return(coefs)
}
myboot = boot(mydata,cof,R=10)
myboot
bootci=boot.ci(myboot,index=1)
library(boot)
mydata=read.csv("project1_data.csv",header=TRUE)
cof=function(data, indices){
dt=data[indices,]
reg=summary(lm(data$y~data$X1+data$X2+data$X3+data$X4+data$X5+data$X6+data$X7+data$X8+data$X9+data$X10))
C1=coef(reg)[2,1]
C2=coef(reg)[3,1]
C3=coef(reg)[4,1]
C4=coef(reg)[5,1]
C5=coef(reg)[6,1]
C6=coef(reg)[7,1]
C7=coef(reg)[8,1]
C8=coef(reg)[9,1]
C9=coef(reg)[10,1]
C10=coef(reg)[11,1]
coefs=c(C1,C2,C3,C4,C5,C6,C7,C8,C9,C10)
return(coefs)
}
myboot = boot(mydata,cof,R=1000)
pdata=read.csv("project1_data.csv",header=TRUE)
attach(pdata)
reg=lm(y~X1+X2+X3+X4+X5+X6+X7+X8+X9+X10)
summary(reg)
install.packages("car")
library(car)
pdata=read.csv("project1_data.csv",header=TRUE)
attach(pdata)
reg=lm(y~X1+X2+X3+X4+X5+X6+X7+X8+X9+X10)
regsum=summary(reg)
myboot=Boot(reg)
myboot
library(car)
pdata=read.csv("project1_data.csv",header=TRUE)
attach(pdata)
reg=lm(y~X1+X2+X3+X4+X5+X6+X7+X8+X9+X10)
regsum=summary(reg)
myboot=Boot(reg, R=10)
myboot
boot.ci(myboot)
summary(myboot)
confint(myboot)
